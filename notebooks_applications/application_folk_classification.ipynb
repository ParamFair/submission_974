{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, set your home directory here:\n",
    "home_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory\n",
    "import os\n",
    "os.chdir(home_dir)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main.models.utils import cv_early_stopping\n",
    "from main.fairness.paramfitter import BetaEMWD\n",
    "from main.fairness.wasserstein import WassersteinBinary\n",
    "from main.fairness.metrics import unfairness\n",
    "\n",
    "# data loaders\n",
    "from main.loaders.loader_pubcov import load_coverage_data, prepare_pubcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some seeds\n",
    "for seed_ in [42, 1029, 3948, 103, 56, 93983838, 828, 1928838, 900, 10]:\n",
    "\n",
    "    data_red = load_coverage_data()\n",
    "\n",
    "    # Create target variable\n",
    "    data_red.loc[:, 'PUBCOV'] = np.where(data_red['PUBCOV'] == 1, 1, 0)\n",
    "    data_red = data_red.assign(low_dummy=np.where(data_red['PINCP'] < 45000, 1, 0))\n",
    "    data_red = data_red.assign(very_low_dummy=np.where(data_red['PINCP'] < 15000, 1, 0))\n",
    "    # Drop column with info\n",
    "    data_red = data_red.drop(columns=['PINCP'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test, transformer = prepare_pubcov(data_red, \n",
    "                                                                    seed=seed_)\n",
    "\n",
    "    # Keep set of original splits (makes it easier to recover\n",
    "    # the orignal variable later on)\n",
    "    X_train_orig = X_train.copy()\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    X_train = np.delete(X_train, 23, axis=1)\n",
    "    X_test = np.delete(X_test, 23, axis=1)\n",
    "            \n",
    "    # Model optimization\n",
    "    # Fit a basic model\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "\n",
    "    cv_results = cv_early_stopping(params=params, \n",
    "                                nfolds=3, \n",
    "                                max_rounds=1000, \n",
    "                                early_stopping_rounds=20, \n",
    "                                X_train=X_train, \n",
    "                                y_train=y_train, \n",
    "                                objective='classification')\n",
    "\n",
    "\n",
    "    # Fit best model (based on boosting iters)\n",
    "    best_res = np.argmax(cv_results['metric'])\n",
    "    best_iter = cv_results['iterations'][best_res]\n",
    "\n",
    "    # Re-Train on whole dataset\n",
    "    data_train_all = lgb.Dataset(data=X_train, \n",
    "                                label=y_train)\n",
    "\n",
    "    best_estimator = lgb.train(params=params,\n",
    "                            train_set=data_train_all, \n",
    "                            num_boost_round=best_iter)\n",
    "\n",
    "    # Run predictions\n",
    "    preds_uncorrected_calib = best_estimator.predict(X_train)\n",
    "    preds_uncorrected_test = best_estimator.predict(X_test)\n",
    "\n",
    "    # Recover full set of sensitive variables\n",
    "    # and set to sensitive variables\n",
    "    # Here: 22 and 23 are the variables we created and we assume that \n",
    "    # 23 is not observed\n",
    "    sens_observed_calib = np.where(X_train_orig[:, 22] > 0, 1, 0)\n",
    "    sens_observed_test = np.where(X_test_orig[:, 22] > 0,1,0) \n",
    "    sens_unobserved_test = np.where(X_test_orig[:,23] > 0,1,0)\n",
    "\n",
    "    # Use a beta model\n",
    "    preds_uncorrected_calib = best_estimator.predict(X_train)\n",
    "    preds_uncorrected_test = best_estimator.predict(X_test)\n",
    "\n",
    "    fairness_trans = WassersteinBinary()\n",
    "    fairness_trans.fit(preds_uncorrected_calib, sens_observed_calib)\n",
    "\n",
    "    nonparam_preds_calib = fairness_trans.transform(preds_uncorrected_calib, sens_observed_calib)\n",
    "    nonparam_preds_test = fairness_trans.transform(preds_uncorrected_test, sens_observed_test)\n",
    "\n",
    "    parametric_sampler = BetaEMWD()\n",
    "    parametric_sampler.fit(X=np.array(nonparam_preds_calib), \n",
    "                            sampling_obs=len(nonparam_preds_calib)*3)\n",
    "\n",
    "    parametric_preds = parametric_sampler.sample(n=len(nonparam_preds_test), \n",
    "                                                    mc_samples=250)\n",
    "\n",
    "    interpolator = interp1d(np.sort(nonparam_preds_test),\n",
    "                            parametric_preds)\n",
    "\n",
    "    param_preds_test = interpolator(nonparam_preds_test)\n",
    "\n",
    "\n",
    "    res_dict = {}\n",
    "\n",
    "    unobs_test = sens_unobserved_test\n",
    "    sens_test = sens_observed_test\n",
    "\n",
    "    for eps_ in [0.0, 0.25,0.5,0.75]:\n",
    "        res_dict[eps_] = {}\n",
    "\n",
    "        cu_ = 0.21\n",
    "            \n",
    "        pred_nonparam_int = eps_*preds_uncorrected_test + (1-eps_)*nonparam_preds_test\n",
    "        pred_param_int = eps_*preds_uncorrected_test + (1-eps_)*param_preds_test\n",
    "\n",
    "        \n",
    "        mse_nonparam = f1_score(y_test, np.where(pred_nonparam_int > cu_, 1, 0))\n",
    "        mse_param = f1_score(y_test, np.where(pred_param_int > cu_, 1, 0))\n",
    "        \n",
    "        unf_np = unfairness(pred_nonparam_int[sens_test == 1], \n",
    "                        pred_nonparam_int[sens_test != 1])\n",
    "        \n",
    "        unf_p = unfairness(pred_param_int[sens_test == 1], \n",
    "                        pred_param_int[sens_test != 1])\n",
    "        \n",
    "        unf_np_u = unfairness(pred_nonparam_int[unobs_test == 1], \n",
    "                        pred_nonparam_int[unobs_test != 1])\n",
    "        \n",
    "        unf_p_u = unfairness(pred_param_int[unobs_test == 1], \n",
    "                        pred_param_int[unobs_test != 1])\n",
    "        \n",
    "        res_dict[eps_]['qrisk_param'] = mse_param\n",
    "        res_dict[eps_]['qrisk_nonparam'] = mse_nonparam\n",
    "\n",
    "        res_dict[eps_]['unfairness_param'] = unf_p\n",
    "        res_dict[eps_]['unfairness_nonparam'] = unf_np\n",
    "\n",
    "        res_dict[eps_]['unfairness_unobs_param'] = unf_p_u\n",
    "        res_dict[eps_]['unfairness_unobs_nonparam'] = unf_np_u\n",
    "\n",
    "    with open(f'data/results/folktabs/dicts/dict_epsiolon{seed_}.pkl', 'wb') as con_:\n",
    "        pickle.dump(res_dict, con_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv_neurips",
   "language": "python",
   "name": "cenv_neurips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
